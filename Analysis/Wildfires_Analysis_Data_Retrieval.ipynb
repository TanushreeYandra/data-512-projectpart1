{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5659e3a2",
   "metadata": {},
   "source": [
    "# DATA 512 - Project Part 1: Common Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74ff72",
   "metadata": {},
   "source": [
    "## Wildfires Analysis - Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84730e81",
   "metadata": {},
   "source": [
    "### Tanushree Yandra, University of Washington, Seattle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c79aa",
   "metadata": {},
   "source": [
    "More and more frequently, summers in the western US have been characterized by wildfires with smoke billowing across multiple western states. There are many proposed causes for this: climate change, US Forestry policy, growing awareness, just to name a few. Regardless of the cause, the impact of wildland fires is widespread. There is a growing body of work pointing to the negative impacts of smoke on health, tourism, property, and other aspects of society. This project analyzes wildfire impacts on the city of Twin Falls, Idaho in the US. The end goal is to be able to inform policy makers, city managers, city councils, or other civic institutions, to make an informed plan for how they could or whether they should make plans to mitigate future impacts from wildfires.\n",
    "\n",
    "Wildland fires within 1250 miles of Twin Falls, Idaho are analyzed for the last 60 years (1963-2020). A smoke estimate is then created to estimate the wildfire smoke impact which is later modeled to make predictions for the next 30 years (until 2049).\n",
    "\n",
    "This section of the notebook discusses the data retrieval process. Two datasets are needed for the analysis - Wildland Fires Dataset to get access to the historical data of wildfires, and the Air Quality Index Dataset to validate our smoke estimate. The data acquisition process for both of these datasets has been explained in detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5fc3c",
   "metadata": {},
   "source": [
    "## 1. Retrieving the Wildland Fires Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448454c9",
   "metadata": {},
   "source": [
    "The [Combined Wildland Fire Datasets for the United States and certain territories, 1800s-Present (combined wildland fire polygons)](https://www.sciencebase.gov/catalog/item/61aa537dd34eb622f699df81) dataset was used to retrieve the historical data of wildfires. This dataset was collected and aggregated by the US Geological Survey, and is relatively well documented. Fire polygons in this dataset are available in ArcGIS and GeoJSON formats.\n",
    "\n",
    "For this analysis, the files were downloaded in the GeoJSON format and a GeoJSON reader was used to read the data. The GeoJSON reader has been sampled from this [module](https://drive.google.com/file/d/1TwCkvdaw0MxJzW7NSDg6XxYQ0dvaS44I/view?usp=sharing). The 'wildfire' module is a user module. This module is available from the course website. The module includes one object, a Reader, that can be used to read the GeoJSON files associated with the wildefire dataset. The module also contains a sample datafile that is GeoJSON compliant and that contains a small number of California wildfires extracted from the main wildfire dataset. This sample code is provided under copyright by the author and cannot be redistributed. For any permissions, please reach out to the author -  Dr. David W. McDonald.\n",
    "\n",
    "While reading the data, geodetic distance computations were simultaneously being made to filter those wildfires that are within 1250 miles of Twin Falls, Idaho. The functions to make these calculations have been sampled from this [sample notebook](https://drive.google.com/file/d/1qNI6hji8CvDeBsnLDAhJXvaqf2gcg8UV/view?usp=sharing). This notebook is licensed under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69820731",
   "metadata": {},
   "source": [
    "### Step 1: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5dcf5",
   "metadata": {},
   "source": [
    "First, we start by importing required modules and packages. For the wildfire module, it is assumed that you have set an appropriate Python path environment variable to point to the location on your machine where you store python user modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4adcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are standard python modules\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# The modules below are not standard Python modules\n",
    "# You will need to install these with pip/pip3 if you do not already have it\n",
    "from pyproj import Transformer, Geod\n",
    "\n",
    "# The module below is the GeoJSON reader that has been sampled\n",
    "from wildfire import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e450cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress the warning statements\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087be639",
   "metadata": {},
   "source": [
    "### Step 2: Reading the GeoJSON Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d587eef",
   "metadata": {},
   "source": [
    "First, load the downloaded GeoJSON data into a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f9be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the GeoJSON file in a JSON object\n",
    "with open('USGS_Wildland_Fire_Combined_Dataset.json', 'r') as json_file:\n",
    "    wildfire_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58c487",
   "metadata": {},
   "source": [
    "We then create a wildfire Reader() object and use it to open the wildfires dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381214c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This opens the named file, reading the file header information\n",
    "# This sets up the file to start reading features uing the next() method\n",
    "reader = Reader.Reader()\n",
    "reader.open(\"USGS_Wildland_Fire_Combined_Dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54daedd5",
   "metadata": {},
   "source": [
    "Once opened, we have access to the header information so we print that to show the structure of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4895d814",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'displayFieldName': '',\n",
       " 'fieldAliases': {'OBJECTID': 'OBJECTID',\n",
       "  'USGS_Assigned_ID': 'USGS Assigned ID',\n",
       "  'Assigned_Fire_Type': 'Assigned Fire Type',\n",
       "  'Fire_Year': 'Fire Year',\n",
       "  'Fire_Polygon_Tier': 'Fire Polygon Tier',\n",
       "  'Fire_Attribute_Tiers': 'Fire Attribute Tiers',\n",
       "  'GIS_Acres': 'GIS_Acres',\n",
       "  'GIS_Hectares': 'GIS_Hectares',\n",
       "  'Source_Datasets': 'Source Datasets',\n",
       "  'Listed_Fire_Types': 'Listed Fire Types',\n",
       "  'Listed_Fire_Names': 'Listed Fire Names',\n",
       "  'Listed_Fire_Codes': 'Listed Fire Codes',\n",
       "  'Listed_Fire_IDs': 'Listed Fire IDs',\n",
       "  'Listed_Fire_IRWIN_IDs': 'Listed Fire IRWIN IDs',\n",
       "  'Listed_Fire_Dates': 'Listed Fire Dates',\n",
       "  'Listed_Fire_Causes': 'Listed Fire Causes',\n",
       "  'Listed_Fire_Cause_Class': 'Listed Fire Cause Class',\n",
       "  'Listed_Rx_Reported_Acres': 'Listed Rx Reported Acres',\n",
       "  'Listed_Map_Digitize_Methods': 'Listed Map Digitize Methods',\n",
       "  'Listed_Notes': 'Listed Notes',\n",
       "  'Processing_Notes': 'Processing Notes',\n",
       "  'Wildfire_Notice': 'Wildfire Notice',\n",
       "  'Prescribed_Burn_Notice': 'Prescribed Burn Notice',\n",
       "  'Wildfire_and_Rx_Flag': 'Wildfire and Rx Flag',\n",
       "  'Overlap_Within_1_or_2_Flag': 'Overlap Within 1 or 2 Years Flag',\n",
       "  'Circleness_Scale': 'Circleness Scale',\n",
       "  'Circle_Flag': 'Circle Flag',\n",
       "  'Exclude_From_Summary_Rasters': 'Exclude From Summary Rasters',\n",
       "  'Shape_Length': 'Shape_Length',\n",
       "  'Shape_Area': 'Shape_Area'},\n",
       " 'geometryType': 'esriGeometryPolygon',\n",
       " 'spatialReference': {'wkid': 102008, 'latestWkid': 102008},\n",
       " 'fields': [{'name': 'OBJECTID',\n",
       "   'type': 'esriFieldTypeOID',\n",
       "   'alias': 'OBJECTID'},\n",
       "  {'name': 'USGS_Assigned_ID',\n",
       "   'type': 'esriFieldTypeInteger',\n",
       "   'alias': 'USGS Assigned ID'},\n",
       "  {'name': 'Assigned_Fire_Type',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Assigned Fire Type',\n",
       "   'length': 100},\n",
       "  {'name': 'Fire_Year',\n",
       "   'type': 'esriFieldTypeSmallInteger',\n",
       "   'alias': 'Fire Year'},\n",
       "  {'name': 'Fire_Polygon_Tier',\n",
       "   'type': 'esriFieldTypeSmallInteger',\n",
       "   'alias': 'Fire Polygon Tier'},\n",
       "  {'name': 'Fire_Attribute_Tiers',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Fire Attribute Tiers',\n",
       "   'length': 30000},\n",
       "  {'name': 'GIS_Acres', 'type': 'esriFieldTypeDouble', 'alias': 'GIS_Acres'},\n",
       "  {'name': 'GIS_Hectares',\n",
       "   'type': 'esriFieldTypeDouble',\n",
       "   'alias': 'GIS_Hectares'},\n",
       "  {'name': 'Source_Datasets',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Source Datasets',\n",
       "   'length': 500000},\n",
       "  {'name': 'Listed_Fire_Types',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Fire Types',\n",
       "   'length': 100000},\n",
       "  {'name': 'Listed_Fire_Names',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Fire Names',\n",
       "   'length': 100000},\n",
       "  {'name': 'Listed_Fire_Codes',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Fire Codes',\n",
       "   'length': 100000},\n",
       "  {'name': 'Listed_Fire_IDs',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Fire IDs',\n",
       "   'length': 30000},\n",
       "  {'name': 'Listed_Fire_IRWIN_IDs',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Fire IRWIN IDs',\n",
       "   'length': 30000},\n",
       "  {'name': 'Listed_Fire_Dates',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Fire Dates',\n",
       "   'length': 500000},\n",
       "  {'name': 'Listed_Fire_Causes',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Fire Causes',\n",
       "   'length': 100000},\n",
       "  {'name': 'Listed_Fire_Cause_Class',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Fire Cause Class',\n",
       "   'length': 100000},\n",
       "  {'name': 'Listed_Rx_Reported_Acres',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Rx Reported Acres',\n",
       "   'length': 10000},\n",
       "  {'name': 'Listed_Map_Digitize_Methods',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Map Digitize Methods',\n",
       "   'length': 100000},\n",
       "  {'name': 'Listed_Notes',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Listed Notes',\n",
       "   'length': 30000},\n",
       "  {'name': 'Processing_Notes',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Processing Notes',\n",
       "   'length': 30000},\n",
       "  {'name': 'Wildfire_Notice',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Wildfire Notice',\n",
       "   'length': 700},\n",
       "  {'name': 'Prescribed_Burn_Notice',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Prescribed Burn Notice',\n",
       "   'length': 700},\n",
       "  {'name': 'Wildfire_and_Rx_Flag',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Wildfire and Rx Flag',\n",
       "   'length': 1000},\n",
       "  {'name': 'Overlap_Within_1_or_2_Flag',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Overlap Within 1 or 2 Years Flag',\n",
       "   'length': 8000},\n",
       "  {'name': 'Circleness_Scale',\n",
       "   'type': 'esriFieldTypeDouble',\n",
       "   'alias': 'Circleness Scale'},\n",
       "  {'name': 'Circle_Flag',\n",
       "   'type': 'esriFieldTypeSmallInteger',\n",
       "   'alias': 'Circle Flag'},\n",
       "  {'name': 'Exclude_From_Summary_Rasters',\n",
       "   'type': 'esriFieldTypeString',\n",
       "   'alias': 'Exclude From Summary Rasters',\n",
       "   'length': 3},\n",
       "  {'name': 'Shape_Length',\n",
       "   'type': 'esriFieldTypeDouble',\n",
       "   'alias': 'Shape_Length'},\n",
       "  {'name': 'Shape_Area',\n",
       "   'type': 'esriFieldTypeDouble',\n",
       "   'alias': 'Shape_Area'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This outputs the header information that was read from the GeoJSON file when it was opened\n",
    "reader.header()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea02cca",
   "metadata": {},
   "source": [
    "### Step 3: Converting Points between Geodetic Coordinate Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b5de1",
   "metadata": {},
   "source": [
    "Now that we are able to read the GeoJSON data, we will now have to define functions to compute the geodetic distance between a wildfire and the city of Twin Falls, Idaho. One of the constraints in doing geodetic computations is that most of the time we need to have our points (the coordinates for places) in the same geographic coordinate system. There are tons and tons of coordinate systems. You can find descriptions of many of them at [EPSG.io](https://epsg.io).\n",
    "\n",
    "Looking at the wildfire header information, we can see fields named \"geometryType\" and \"spatialReference\". This looks like:\n",
    "\n",
    "        \"geometryType\": \"esriGeometryPolygon\",\n",
    "        \"spatialReference\": {\n",
    "            \"wkid\": 102008,\n",
    "            \"latestWkid\": 102008\n",
    "        },\n",
    "\n",
    "This indicates that the geometry of our wildfire data are generic polygons and that they are expressed in a coordinate system with the well-known ID (WKID) 102008. This coordinate system is also known as [ESRI:102008](https://epsg.io/102008)\n",
    "\n",
    "The most commonly used geographic coordinate system however is 'WGS84', that is sometimes called 'decimal degrees' (DD). This decimal degrees system has an official name (or WKID) of [EPSG:4326](https://epsg.io/4326). The coordinates of our city - Twin Falls, Idaho are stored in this format.\n",
    "\n",
    "Thus, in order to compute the distance, we will first need to convert the coordinates to the Decimal Degrees system. We are going to do this by taking the geometry of a fire feature, extracting the largest ring (i.e., the largest boundary of the fire) and converting all of the points in that ring from the ESRI:102008 coordinate system to EPSG:4326 coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc84fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "\n",
    "# The function takes one parameter, a list of ESRI:102008 coordinates that will be transformed to EPSG:4326\n",
    "# The function returns a list of coordinates in EPSG:4326\n",
    "\n",
    "def convert_ring_to_epsg4326(ring_data=None):\n",
    "    \n",
    "    # Initiate an empty list\n",
    "    converted_ring = list()\n",
    "\n",
    "    # Pyproj transformer is used to convert coordinates from ESRI:102008 to EPSG:4326\n",
    "    to_epsg4326 = Transformer.from_crs(\"ESRI:102008\",\"EPSG:4326\")\n",
    "    \n",
    "    # Run through the list transforming each ESRI:102008 x,y coordinate into a decimal degree lat,lon\n",
    "    for coord in ring_data:\n",
    "        lat,lon = to_epsg4326.transform(coord[0],coord[1])\n",
    "        new_coord = lat,lon\n",
    "        # Append the converted coordinates to the list\n",
    "        converted_ring.append(new_coord)\n",
    "    return converted_ring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe88886",
   "metadata": {},
   "source": [
    "### Step 4: Computing the Geodetic Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5c8a3",
   "metadata": {},
   "source": [
    "The basic problem is knowing how far away a fire is from some particular location. One issue is that fires are irregularly shaped so the actual answer to that is a bit dependent upon the exact shape and how you want to think about the notion of 'distance'. For example, should we just find the closest point on the perimiter of a fire and call that the distance? Maybe we should find the centroid of the region, identify that as a geolocation (coordinate) and then calculate the distance to that? We can come up with numerous other ways.\n",
    "\n",
    "For this analysis, the average distance of all perimeter points to Twin Falls, Idaho was considered. This is not quite what the centroid would be, but it is probably fairly close. This method was chosen because for some very large fires, including just the shortest distance seemed a little biased. Average distance felt like a more rounded and appropriate approach. The function below shows how this distance can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74cabdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "\n",
    "# The function takes two parameters\n",
    "# place - coordinates as a list or tuple with two items, (lat,lon) in decimal degrees EPSG:4326\n",
    "# ring_data - a list of decimal degree coordinates for the fire boundary\n",
    "\n",
    "# The function returns the average miles from boundary to the place\n",
    "\n",
    "def average_distance_from_place_to_fire_perimeter(place=None,ring_data=None):\n",
    "    \n",
    "    # Convert the ring data to the right coordinate system\n",
    "    ring = convert_ring_to_epsg4326(ring_data)\n",
    "    \n",
    "    # Create a epsg4326 compliant object - which is what the WGS84 ellipsoid is\n",
    "    geodcalc = Geod(ellps='WGS84')\n",
    "    \n",
    "    # Create a list to store our results\n",
    "    distances_in_meters = list()\n",
    "    \n",
    "    # Run through each point in the converted ring data\n",
    "    for point in ring:\n",
    "        \n",
    "        # Calculate the distance\n",
    "        d = geodcalc.inv(place[1],place[0],point[1],point[0])\n",
    "        distances_in_meters.append(d[2])\n",
    "\n",
    "    # Convert meters to miles\n",
    "    distances_in_miles = [meters*0.00062137 for meters in distances_in_meters]\n",
    "    \n",
    "    # The ring requires that the first and last coordinates be identical to close the polygon region\n",
    "    # We thus remove one of them so that we don't bias our average by having two of the same point\n",
    "    distances_in_miles_no_dup = distances_in_miles[1:]\n",
    "    \n",
    "    # Now, compute the average miles\n",
    "    average = sum(distances_in_miles_no_dup)/len(distances_in_miles_no_dup)\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720ed04",
   "metadata": {},
   "source": [
    "### Step 5: Generating the Final Wildfires Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f1191",
   "metadata": {},
   "source": [
    "In this section, we use the Reader() object and the next() method to read all the wildfire features in the dataset. For this analysis, every 'feature' is a wildfire. While we read individual 'features' we also access the geometry of each feature to get the 'ring' boundary of that specific fire - which is a list of geodetic coordinates. These coordinates allow us to compute the distance between the wildfire and our city of interest.\n",
    "\n",
    "Some of the wildfire features contain 'curveRings' in the feature geometry instead of 'rings'. Such wildfires are very less (< ~50) and thus have been ignored for the purpose of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a82833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coordinates for your city of interest (Twin Falls, Idaho in our case)\n",
    "city_latlon = [42.562786, -114.460503]\n",
    "\n",
    "# Initiate an empty list to store the wildfires data\n",
    "final_wildfire_data = []\n",
    "# Set the pointer to the first feature of the dataset\n",
    "wildfire_feature = reader.next()\n",
    "\n",
    "# Iterate through every feature until it return an empty value which indicates the file has ended\n",
    "while wildfire_feature is not None:\n",
    "    \n",
    "    # Extract the fire year from the attributes of the feature\n",
    "    fire_year = wildfire_feature['attributes']['Fire_Year']\n",
    "    \n",
    "    # Ignore wildfires before the year 1963\n",
    "    if fire_year >= 1963:\n",
    "        \n",
    "        # Get the geometry for the feature we pulled from the feature_list\n",
    "        wildfire_geometry = wildfire_feature['geometry']\n",
    "        \n",
    "        # Ignore features that have 'curveRings' instead of 'rings'\n",
    "        if list(wildfire_feature['geometry'].keys())[0] == 'curveRings':\n",
    "            wildfire_feature = reader.next()\n",
    "            continue;\n",
    "\n",
    "        # Get all the coordinates for the fire boundary\n",
    "        ring_data = wildfire_feature['geometry']['rings'][0]\n",
    "        \n",
    "        # Compute the average distance between the fire boundary and Twin Falls, Idaho\n",
    "        distance = average_distance_from_place_to_fire_perimeter(city_latlon,ring_data)\n",
    "        \n",
    "        # Only consider those wildfires that are within 1250 miles of Twin Falls, Idaho\n",
    "        if distance <= 1250:\n",
    "            \n",
    "            # Save the distance value in the feature attributes and append the feature to the list\n",
    "            wildfire_feature['attributes']['Distance'] = distance\n",
    "            final_wildfire_data.append(wildfire_feature)\n",
    "    \n",
    "    # Move to the next feature\n",
    "    wildfire_feature = reader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e5e33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84319\n"
     ]
    }
   ],
   "source": [
    "# Look at the total features in the dataset\n",
    "len(final_wildfire_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f862d4",
   "metadata": {},
   "source": [
    "Thus, we have a total of 84319 wildfires in the period 1963-2020 where all the wildfires are within 1250 miles of Twin Falls, Idaho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3b0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of dictionaries in a json file\n",
    "with open('final_wildfire_data.json', 'w') as json_file:\n",
    "    json.dump(final_wildfire_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e668dfd",
   "metadata": {},
   "source": [
    "## 2. Retrieving US EPA Air Quality Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf396d7",
   "metadata": {},
   "source": [
    "The code below illustrates how to request data from the US Environmental Protection Agency (EPA) Air Quality Service (AQS) API. This is a historical API and does not provide real-time air quality data. The [documentation](https://aqs.epa.gov/aqsweb/documents/data_api.html) for the API provides definitions of the different call parameter and examples of the various calls that can be made to the API.\n",
    "\n",
    "The code for the data retrieval has been sampled from this [sample notebook](https://drive.google.com/file/d/1bxl9qrb_52RocKNGfbZ5znHVqFDMkUzf/view?usp=sharing) which is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). The sampled code works systematically through requesting an API key, using 'list' to get various IDs and parameter values, and using 'daily summary' to get summary data that meets specific condistions. Changing values to explore the results of the API is probably useful, but that will result in some explanations being out of sync with the outputs.\n",
    "\n",
    "The US EPA was created in the early 1970's. The EPA reports that they only started broad based monitoring with standardized quality assurance procedures in the 1980's. Many counties will have data starting somewhere between 1983 and 1988. However, some counties still do not have any air quality monitoring stations. The API helps resolve this by providing calls to search for monitoring stations and data using either station ids, or a county designation or a geographic bounding box. The code below shows how the Air Quality Index or AQI data can be retrieved using a county designation. Some [additional information on the Air Quality System can be found in the EPA FAQ](https://www.epa.gov/outdoor-air-quality-data/frequent-questions-about-airdata) on the system.\n",
    "\n",
    "The end goal of this code is to get to the AQI data for the city of Twin Falls, Idaho. The AQI index is meant to tell us something about how healthy or clean the air is on any day. The AQI is actually a somewhat complex measure. To understand what parameters go into calculating the AQI, check [how to calculate the AQI](https://www.airnow.gov/sites/default/files/2020-05/aqi-technical-assistance-document-sept2018.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa27bf0",
   "metadata": {},
   "source": [
    "### Step 1: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a4ebe",
   "metadata": {},
   "source": [
    "First, we start by importing required modules and packages, and defining some constants that will be used later for the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666ca0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are standard python modules\n",
    "import json, time\n",
    "import numpy as np\n",
    "\n",
    "# The modules below are not standard Python modules\n",
    "# You will need to install these with pip/pip3 if you do not already have it\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d1daa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "API_REQUEST_URL = 'https://aqs.epa.gov/data/api'\n",
    "\n",
    "# These are 'actions' we can ask the API to take or requests that we can make of the API\n",
    "# Sign-up request - generally only performed once - unless you lose your key\n",
    "API_ACTION_SIGNUP = '/signup?email={email}'\n",
    "\n",
    "# List actions provide information on API parameter values that are required by other actions/requests\n",
    "API_ACTION_LIST_CLASSES = '/list/classes?email={email}&key={key}'\n",
    "API_ACTION_LIST_PARAMS = '/list/parametersByClass?email={email}&key={key}&pc={pclass}'\n",
    "API_ACTION_LIST_SITES = '/list/sitesByCounty?email={email}&key={key}&state={state}&county={county}'\n",
    "\n",
    "# Monitor actions are requests for monitoring stations that meet specific criteria\n",
    "API_ACTION_MONITORS_COUNTY = '''/monitors/byCounty?email={email}&key={key}&param={param}&\n",
    "bdate={begin_date}&edate={end_date}&state={state}&county={county}'''\n",
    "API_ACTION_MONITORS_BOX = '''/monitors/byBox?email={email}&key={key}&param={param}&bdate={begin_date}\n",
    "&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'''\n",
    "\n",
    "# Summary actions are requests for summary data. These are for daily summaries\n",
    "API_ACTION_DAILY_SUMMARY_COUNTY = '''/dailyData/byCounty?email={email}&key={key}&param={param}&\n",
    "bdate={begin_date}&edate={end_date}&state={state}&county={county}'''\n",
    "API_ACTION_DAILY_SUMMARY_BOX = '''/dailyData/byBox?email={email}&key={key}&param={param}&\n",
    "bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'''\n",
    "\n",
    "# It is always nice to be respectful of a free data resource\n",
    "# We are going to observe a 100 requests per minute limit - which is fairly nice\n",
    "# Assuming roughly a 2ms latency on the API and network\n",
    "API_LATENCY_ASSUMED = 0.002       \n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# This is a template that covers most of the parameters for the actions we might take\n",
    "AQS_REQUEST_TEMPLATE = {\n",
    "    \"email\":      \"\",     \n",
    "    \"key\":        \"\",      \n",
    "    \"state\":      \"\",     # the two digit state FIPS # as a string\n",
    "    \"county\":     \"\",     # the three digit county FIPS # as a string\n",
    "    \"begin_date\": \"\",     # the start of a time window in YYYYMMDD format\n",
    "    \"end_date\":   \"\",     # the end of a time window in YYYYMMDD format (both dates must have same year)\n",
    "    \"minlat\":    0.0,\n",
    "    \"maxlat\":    0.0,\n",
    "    \"minlon\":    0.0,\n",
    "    \"maxlon\":    0.0,\n",
    "    \"param\":     \"\",      # a list of comma separated 5 digit codes, max 5 codes requested\n",
    "    \"pclass\":    \"\"       # parameter class is only used by the List calls\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d169d",
   "metadata": {},
   "source": [
    "### Step 2: Making a Sign-Up Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d54b8",
   "metadata": {},
   "source": [
    "Before you can use the API you need to request a key. You will use an email address to make the request. The EPA then sends a confirmation email link and a 'key' that you use for all other requests.\n",
    "\n",
    "You only need to sign-up once, unless you want to invalidate your current key (by getting a new key) or you lose your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2684831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "\n",
    "# This implements the sign-up request\n",
    "# The parameters are standardized so that this function definition matches all of the others\n",
    "# The easiest way to call this is to simply call this function with your preferred email address.\n",
    "\n",
    "def request_signup(email_address = None,\n",
    "                   endpoint_url = API_REQUEST_URL, \n",
    "                   endpoint_action = API_ACTION_SIGNUP, \n",
    "                   request_template = AQS_REQUEST_TEMPLATE,\n",
    "                   headers = None):\n",
    "    \n",
    "    # If you don't have access to this email addres, things might go badly for you\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address        \n",
    "    if not request_template['email']: \n",
    "        raise Exception(\"Must supply an email address to call 'request_signup()'\")\n",
    "    \n",
    "    # Compose the signup url\n",
    "    # Create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # Make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in case an exception occurs\n",
    "        # During the request processing, throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ef752ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SIGNUP request is only to be done once, to request a key\n",
    "# A key is sent to that email address and needs to be confirmed with a click\n",
    "# This code should be commented out after you've made your key request\n",
    "# This will make sure you don't accidentally make a new sign-up request\n",
    "\n",
    "#response = request_signup(\"tyandra@uw.edu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d223708",
   "metadata": {},
   "source": [
    "Assign your email address to \"USERNAME\" and your key to \"APIKEY\" as constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2c751f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have the signup email, we can define two constants\n",
    "# USERNAME - the email address you sent the EPA asking for access to the API during sign-up\n",
    "# APIKEY - This should be the authorization key they sent you\n",
    "\n",
    "# You can specify these as constants for your own use\n",
    "# Just don't distribute the notebook without removing the constants\n",
    "\n",
    "#USERNAME = \"<the_email_address_you_sent_on_signup>\"\n",
    "#APIKEY = \"<the_key_the_EPA_sent_you_in_email>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990fb90",
   "metadata": {},
   "source": [
    "### Step 3: Making a List Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed53ac",
   "metadata": {},
   "source": [
    "Once you have a key, the next thing is to get information about the different types of air quality monitoring (sensors) and the different places where we might find air quality stations. The monitoring system is complex and changes all the time. The EPA implementation allows an API user to find changes to monitoring sites and sensors by making requests - maybe monthly, or daily. This API approach is probably better than having the EPA publish documentation that may be out of date as soon as it hits a web page. The one problem here is that some of the responses rely on jargon or terms-of-art. That is, one needs to know a bit about the way atmospheric sciece works to understand some of the terms. Good thing we can use the web to search for terms we don't know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e7b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "\n",
    "# This implements the list request\n",
    "# There are several versions of the list request that only require email and key\n",
    "# This code sets the default action/requests to list the groups or parameter class descriptors\n",
    "# This enables to request the 5 digit codes for individual air quality measures using the param request\n",
    "\n",
    "def request_list_info(email_address = None, key = None,\n",
    "                      endpoint_url = API_REQUEST_URL, \n",
    "                      endpoint_action = API_ACTION_LIST_CLASSES, \n",
    "                      request_template = AQS_REQUEST_TEMPLATE,\n",
    "                      headers = None):\n",
    "    \n",
    "    #  Make sure we have email and key - at least\n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    \n",
    "    # For the basic request we need an email address and a key\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_list_info()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_list_info()'\")\n",
    "\n",
    "    # Compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # Make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4f327fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"AIRNOW MAPS\",\n",
      "        \"value_represented\": \"The parameters represented on AirNow maps (88101, 88502, and 44201)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"ALL\",\n",
      "        \"value_represented\": \"Select all Parameters Available\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"AQI POLLUTANTS\",\n",
      "        \"value_represented\": \"Pollutants that have an AQI Defined\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CORE_HAPS\",\n",
      "        \"value_represented\": \"Urban Air Toxic Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CRITERIA\",\n",
      "        \"value_represented\": \"Criteria Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CSN DART\",\n",
      "        \"value_represented\": \"List of CSN speciation parameters to populate the STI DART tool\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"FORECAST\",\n",
      "        \"value_represented\": \"Parameters routinely extracted by AirNow (STI)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"HAPS\",\n",
      "        \"value_represented\": \"Hazardous Air Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"IMPROVE CARBON\",\n",
      "        \"value_represented\": \"IMPROVE Carbon Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"IMPROVE_SPECIATION\",\n",
      "        \"value_represented\": \"PM2.5 Speciated Parameters Measured at IMPROVE sites\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"MET\",\n",
      "        \"value_represented\": \"Meteorological Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"NATTS CORE HAPS\",\n",
      "        \"value_represented\": \"The core list of toxics of interest to the NATTS program.\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"NATTS REQUIRED\",\n",
      "        \"value_represented\": \"Required compounds to be collected in the National Air Toxics Network\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PAMS\",\n",
      "        \"value_represented\": \"Photochemical Assessment Monitoring System\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PAMS_VOC\",\n",
      "        \"value_represented\": \"Volatile Organic Compound subset of the PAMS Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM COARSE\",\n",
      "        \"value_represented\": \"PM between 2.5 and 10 micrometers\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM10 SPECIATION\",\n",
      "        \"value_represented\": \"PM10 Speciated Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM2.5 CONT NONREF\",\n",
      "        \"value_represented\": \"PM2.5 Continuous, Nonreference Methods\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM2.5 MASS/QA\",\n",
      "        \"value_represented\": \"PM2.5 Mass and QA Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SCHOOL AIR TOXICS\",\n",
      "        \"value_represented\": \"School Air Toxics Program Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION\",\n",
      "        \"value_represented\": \"PM2.5 Speciated Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION CARBON\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Carbon Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION CATION/ANION\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Cation/Anion Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION METALS\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Metal Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"UATMP CARBONYL\",\n",
      "        \"value_represented\": \"Urban Air Toxics Monitoring Program Carbonyls\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"UATMP VOC\",\n",
      "        \"value_represented\": \"Urban Air Toxics Monitoring Program VOCs\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"VOC\",\n",
      "        \"value_represented\": \"Volatile organic compounds\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# The default should get us a list of the various groups or classes of sensors\n",
    "# These classes are user defined names for clustors of sensors\n",
    "# We need a class name to start getting down to the sensor ID\n",
    "# Each sensor type has an ID number\n",
    "# We'll eventually need those ID numbers to be able to request values from that specific sensor\n",
    "\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "\n",
    "# Make a list request\n",
    "response = request_list_info(request_template=request_data)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d90822",
   "metadata": {},
   "source": [
    "We are interested in getting to something that might be the Air Quality Index (AQI). You see this reported on the news - often around smog values, but also when there is smoke in the sky. The AQI is a complex measure of different gases and particles in the air (dust, dirt, ash, etc.).\n",
    "\n",
    "From the list produced by our 'list/Classes' request above, it looks like there is a class of sensors called \"AQI POLLUTANTS\". Let's try to get a list of those specific sensors and see what we can get from those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798e31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find the sensor IDs that make up the class (group) we identified above\n",
    "# The one that looks to be associated with the Air Quality Index is \"AQI POLLUTANTS\"\n",
    "# We'll use that to make another list request\n",
    "\n",
    "AQI_PARAM_CLASS = \"AQI POLLUTANTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bb953e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"42101\",\n",
      "        \"value_represented\": \"Carbon monoxide\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"42401\",\n",
      "        \"value_represented\": \"Sulfur dioxide\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"42602\",\n",
      "        \"value_represented\": \"Nitrogen dioxide (NO2)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"44201\",\n",
      "        \"value_represented\": \"Ozone\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"81102\",\n",
      "        \"value_represented\": \"PM10 Total 0-10um STP\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"88101\",\n",
      "        \"value_represented\": \"PM2.5 - Local Conditions\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"88502\",\n",
      "        \"value_represented\": \"Acceptable PM2.5 AQI & Speciation Mass\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Structure a request to get the sensor IDs associated with the AQI\n",
    "\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "\n",
    "# Here we specify that we want this 'pclass' or parameter classs\n",
    "request_data['pclass'] = AQI_PARAM_CLASS\n",
    "\n",
    "# Make a list request\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_PARAMS)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188acb2b",
   "metadata": {},
   "source": [
    "We should now have (above) a response containing a set of sensor ID numbers. The list should include the sensor numbers as well as a description or name for each sensor.\n",
    "\n",
    "The EPA AQS API has limits on some call parameters. Specifically, when we request data for sensors we can only specify a maximum of 5 different sensor values to return. This means we cannot get all of the Air Quality Index parameters in one request for data. We have to break it up. This can be achieved by breaking the request into two logical groups, the AQI sensors that sample gasses and the AQI sensors that sample particles in the air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bacc970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the sensor codes, we can create a parameter list as defined by the AQS API\n",
    "# It turns out that we want all of these measures for AQI\n",
    "# But we need to have two different param constants to get all seven of the code types\n",
    "# We can only have a max of 5 sensors/values request per param\n",
    "# Thus, they are divided into two group - gaseous and particulate\n",
    "\n",
    "# Gaseous AQI pollutants CO, SO2, NO2, and O2\n",
    "AQI_PARAMS_GASEOUS = \"42101,42401,42602,44201\"\n",
    "\n",
    "# Particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eacddd",
   "metadata": {},
   "source": [
    "Air quality monitoring stations are located all over the US at different locations. We will need some sample locations to experiment with different locations to see what kinds of values come back from different sensor requests.\n",
    "\n",
    "This list includes the [FIPS](https://www.census.gov/library/reference/code-lists/ansi.html) number for the state and county as a 5 digit string. This format, the 5 digit string, is a 'old' format that is still widely used. There are new codes that may eventually be adopted for the US government information systems. But FIPS is currently what the AQS uses, so that's what is in the list as the constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afd0634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the FIPS code and latitude, longitude details of Twin Falls, Idaho in a dictionary\n",
    "\n",
    "CITY = {'city' : 'Twin Falls',\n",
    "        'county' : 'Twin Falls',\n",
    "        'state'  : 'Idaho',\n",
    "        'fips'   : '16083',\n",
    "        'latlon' : [42.562786, -114.460503]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8ceac",
   "metadata": {},
   "source": [
    "Given our city constants, we can now find which monitoring locations are nearby. This code uses the county to define the area we are interested in. You can get the EPA to list their monitoring stations by county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b8aee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"0001\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0002\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0003\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0004\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0005\",\n",
      "        \"value_represented\": \"ROOF OF LARGE RETAIL STORE IN CENTRAL TWIN FALLS\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0006\",\n",
      "        \"value_represented\": \"BACKGROUND SITE IN REMOTE HIGH DESERT AREA WEST OF ROGERSON, ID\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0007\",\n",
      "        \"value_represented\": \"Twin Falls\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0009\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0010\",\n",
      "        \"value_represented\": \"MONITOR IS ON ROOF OF SMITH'S FOOD STORE, IN RESIDENTIAL SECTION.\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"1001\",\n",
      "        \"value_represented\": null\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# This list request gives a list of all monitoring stations in the county specified by CITY dictionary\n",
    "\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['state'] = CITY['fips'][:2]   # the first two digits (characters) of FIPS is the state code\n",
    "request_data['county'] = CITY['fips'][2:]  # the last three digits (characters) of FIPS is the county code\n",
    "\n",
    "# Make a list request\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_SITES)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa3767",
   "metadata": {},
   "source": [
    "The above response gives us a list of monitoring stations. Each monitoring station has a unique \"code\" which is a string number, and, sometimes, a description. The description seems to be something about where the monitoring station is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f1da1",
   "metadata": {},
   "source": [
    "### Step 4: Making a Daily Summary Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011206a",
   "metadata": {},
   "source": [
    "The function below is designed to encapsulate requests to the EPA AQS API. When calling the function, one should create/copy a parameter template, then initialize that template with values that won't change with each call. Then on each call, simply pass in the parameters that need to change, like date ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ad35e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION\n",
    "\n",
    "# This implements the daily summary request\n",
    "# Daily summary provides a summary value for each sensor from the start date to the end date\n",
    "# This can be called with a mixture of a defined parameter dictionary, or with function parameters\n",
    "# If function parameters are provided, they take precedence over parameters from the request template\n",
    "\n",
    "def request_daily_summary(email_address = None, key = None, param=None,\n",
    "                          begin_date = None, end_date = None, fips = None,\n",
    "                          endpoint_url = API_REQUEST_URL, \n",
    "                          endpoint_action = API_ACTION_DAILY_SUMMARY_COUNTY, \n",
    "                          request_template = AQS_REQUEST_TEMPLATE,\n",
    "                          headers = None):\n",
    "    \n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]            \n",
    "\n",
    "    # Make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_daily_summary()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_daily_summary()'\")\n",
    "    if not request_template['param']: \n",
    "        raise Exception(\"Must supply param values to call 'request_daily_summary()'\")\n",
    "    if not request_template['begin_date']: \n",
    "        raise Exception(\"Must supply a begin_date to call 'request_daily_summary()'\")\n",
    "    if not request_template['end_date']: \n",
    "        raise Exception(\"Must supply an end_date to call 'request_daily_summary()'\")\n",
    "    # Note we're not validating FIPS fields because not all of the daily summary actions require the FIPS numbers\n",
    "        \n",
    "    # Compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # Make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in case an exception occurs\n",
    "        # During the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa96dc",
   "metadata": {},
   "source": [
    "### Step 5: Generating the Final AQI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d10f0a",
   "metadata": {},
   "source": [
    "During initial analysis, it was found that the Gaseuous AQI data was missing for the city of Twin Falls, Idaho. It was also found that the AQI data for monitoring stations in Twin Falls county is available from 1986 onwards. Thus, even though our wildfires dataset ranges from the year 1963 to 2020, the AQI data is available from 1986 only.\n",
    "\n",
    "Apart from that, since we are making a \"daily\" summary request, the AQI values need to be estimated on a yearly basis somehow. This can be done by taking an average of all the AQI data for that year or picking the maximum value as well. Since this analysis is concerned about the potential extreme impacts of wildfires on air quality and how high pollution events correlate with smoke estimates, considering the maximum AQI for each year felt more suitable. The document for the [Reporting of Daily Air Quality](https://www.airnow.gov/sites/default/files/2020-05/aqi-technical-assistance-document-sept2018.pdf) sheds some light on how AQI can be calculated across different pollutants. It suggests picking the maximum AQI value when multiple pollutants contribute towards the AQI. This thus further strengthens our approach of picking the maximum AQI value instead of average.\n",
    "\n",
    "Finally, instead of collecting the entire year's data to estimate the annual AQI, the data is pulled only for the period of fire season. The annual fire season is said to run from May 1st through October 31st. Since our aim is to understand the smoke estimate's relation with AQI levels, picking the maximum AQI level from the fire season period every year seemed appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd77eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dataframe with year and AQI columns\n",
    "aqi_df = pd.DataFrame(columns=['Year', 'AQI'])\n",
    "\n",
    "# Initialize the year range and all the request parameters\n",
    "year = list(np.arange(1986, 2021, 1))\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['state'] = CITY['fips'][:2]\n",
    "request_data['county'] = CITY['fips'][2:]\n",
    "request_data['param'] = AQI_PARAMS_PARTICULATES\n",
    "\n",
    "# Iterate through every year\n",
    "for y in year:\n",
    "    \n",
    "    # Initialize the start and end date based on the fire season\n",
    "    begin_date = str(y)+'0501'\n",
    "    end_date = str(y)+'1031'\n",
    "    \n",
    "    # Request daily summary data for the fire season for a specific year\n",
    "    particulate_aqi = request_daily_summary(request_template=request_data, begin_date=begin_date, \n",
    "                                            end_date=end_date)\n",
    "    # Set maximum AQI to zero\n",
    "    max_aqi = 0\n",
    "    \n",
    "    # Iterate through the daily summary data from different monitoring stations\n",
    "    for data in particulate_aqi[\"Data\"]:\n",
    "        \n",
    "        # Check if the AQI value is greater than the current maximum AQI value\n",
    "        if \"aqi\" in list(data.keys()):\n",
    "            aqi = data[\"aqi\"]\n",
    "            if aqi is not None and aqi > max_aqi:\n",
    "                max_aqi = aqi\n",
    "    \n",
    "    # Create a new row with the year and maximum AQI\n",
    "    new_row = {'Year': y, 'AQI': max_aqi}\n",
    "    # Append the new row to the DataFrame\n",
    "    aqi_df =  pd.concat([aqi_df, pd.DataFrame([new_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55223184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1989</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1991</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1995</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1996</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1997</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1998</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2001</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2002</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2003</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2004</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2005</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2006</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2007</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2008</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2009</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2011</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2012</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2013</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2015</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2016</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2017</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  AQI\n",
       "0   1986   64\n",
       "1   1987   75\n",
       "2   1988   62\n",
       "3   1989   64\n",
       "4   1990   68\n",
       "5   1991   72\n",
       "6   1992    0\n",
       "7   1993    0\n",
       "8   1994    0\n",
       "9   1995   81\n",
       "10  1996   57\n",
       "11  1997   39\n",
       "12  1998   40\n",
       "13  1999   50\n",
       "14  2000   56\n",
       "15  2001   65\n",
       "16  2002   60\n",
       "17  2003   61\n",
       "18  2004   61\n",
       "19  2005   65\n",
       "20  2006   79\n",
       "21  2007   79\n",
       "22  2008   79\n",
       "23  2009   57\n",
       "24  2010  131\n",
       "25  2011   54\n",
       "26  2012  155\n",
       "27  2013   75\n",
       "28  2014    0\n",
       "29  2015  141\n",
       "30  2016   64\n",
       "31  2017  153\n",
       "32  2018  151\n",
       "33  2019   71\n",
       "34  2020  160"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the final dataframe\n",
    "aqi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e362f11",
   "metadata": {},
   "source": [
    "It can be found that the AQI Data is missing for the years 1992, 1993, 1994 and 2014. These values are first replaced with an empty value instead of zero, and filled with the rolling average value of the five previous AQI values. While these values may not be accurate, considering five previous values' rolling average might be a reasonal estimate for the missing AQI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26013a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeroes with empty values\n",
    "aqi_df['AQI'] = aqi_df['AQI'].replace(0, None)\n",
    "\n",
    "# Fill the empty values with the rolling average of the five previous AQI values\n",
    "filled_column = aqi_df['AQI'].fillna(aqi_df['AQI'].rolling(5, min_periods=1).mean())\n",
    "\n",
    "# Update the original column in the DataFrame\n",
    "aqi_df['AQI'] = filled_column\n",
    "aqi_df['AQI'] = aqi_df['AQI'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94e66b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1989</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1991</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1992</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1993</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1994</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1995</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1996</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1997</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1998</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2001</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2002</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2003</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2004</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2005</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2006</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2007</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2008</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2009</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2011</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2012</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2013</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2014</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2015</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2016</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2017</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  AQI\n",
       "0   1986   64\n",
       "1   1987   75\n",
       "2   1988   62\n",
       "3   1989   64\n",
       "4   1990   68\n",
       "5   1991   72\n",
       "6   1992   66\n",
       "7   1993   68\n",
       "8   1994   70\n",
       "9   1995   81\n",
       "10  1996   57\n",
       "11  1997   39\n",
       "12  1998   40\n",
       "13  1999   50\n",
       "14  2000   56\n",
       "15  2001   65\n",
       "16  2002   60\n",
       "17  2003   61\n",
       "18  2004   61\n",
       "19  2005   65\n",
       "20  2006   79\n",
       "21  2007   79\n",
       "22  2008   79\n",
       "23  2009   57\n",
       "24  2010  131\n",
       "25  2011   54\n",
       "26  2012  155\n",
       "27  2013   75\n",
       "28  2014  103\n",
       "29  2015  141\n",
       "30  2016   64\n",
       "31  2017  153\n",
       "32  2018  151\n",
       "33  2019   71\n",
       "34  2020  160"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the updated dataframe\n",
    "aqi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd36330",
   "metadata": {},
   "source": [
    "It can be observed that the missing AQI data has now been replaced with the rolling average values of the previous five AQI measurements. The dataset is now complete for the years 1986-2020, and can be exported to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b5016ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final AQI dataframe to a CSV file\n",
    "aqi_df.to_csv('Yearly_AQI_Data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
